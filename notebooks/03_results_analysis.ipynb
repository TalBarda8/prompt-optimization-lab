{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis\n",
    "\n",
    "This notebook analyzes experimental results and generates visualizations.\n",
    "\n",
    "**Contents:**\n",
    "1. Load experimental results\n",
    "2. Analyze technique performance\n",
    "3. Statistical comparison\n",
    "4. Generate visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "from visualization import (\n",
    "    plot_accuracy_comparison,\n",
    "    plot_loss_comparison,\n",
    "    plot_confidence_intervals,\n",
    "    plot_technique_rankings,\n",
    ")\n",
    "from pipeline.statistics import StatisticalValidator\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Results\n",
    "\n",
    "**Note:** This section assumes results from a completed experiment.\n",
    "If you haven't run an experiment yet, run:\n",
    "```bash\n",
    "python main.py run-experiment\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results (update path if needed)\n",
    "results_path = '../results/experiment_results.json'\n",
    "\n",
    "try:\n",
    "    with open(results_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    print(\"✅ Results loaded successfully!\")\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Model: {results['config']['llm_model']}\")\n",
    "    print(f\"  Techniques: {', '.join(results['config']['techniques'])}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Results file not found. Please run an experiment first.\")\n",
    "    print(\"   Command: python main.py run-experiment\")\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mock Data for Demonstration\n",
    "\n",
    "Since we may not have run actual experiments yet, let's create mock data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock results for demonstration\n",
    "np.random.seed(42)\n",
    "\n",
    "techniques = [\n",
    "    'baseline',\n",
    "    'chain_of_thought',\n",
    "    'chain_of_thought_plus_plus',\n",
    "    'react',\n",
    "    'tree_of_thoughts',\n",
    "    'role_based',\n",
    "    'few_shot',\n",
    "]\n",
    "\n",
    "# Generate mock accuracy data\n",
    "mock_accuracy = {\n",
    "    'baseline': 0.72,\n",
    "    'chain_of_thought': 0.85,\n",
    "    'chain_of_thought_plus_plus': 0.89,\n",
    "    'react': 0.83,\n",
    "    'tree_of_thoughts': 0.87,\n",
    "    'role_based': 0.78,\n",
    "    'few_shot': 0.81,\n",
    "}\n",
    "\n",
    "# Generate mock loss data (lower is better)\n",
    "mock_loss = {\n",
    "    'baseline': 0.38,\n",
    "    'chain_of_thought': 0.25,\n",
    "    'chain_of_thought_plus_plus': 0.20,\n",
    "    'react': 0.27,\n",
    "    'tree_of_thoughts': 0.23,\n",
    "    'role_based': 0.32,\n",
    "    'few_shot': 0.29,\n",
    "}\n",
    "\n",
    "# Generate mock distributions\n",
    "mock_accuracy_dist = {\n",
    "    tech: list(np.random.normal(acc, 0.05, 30)) \n",
    "    for tech, acc in mock_accuracy.items()\n",
    "}\n",
    "\n",
    "print(\"Mock data generated for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy comparison\n",
    "fig = plot_accuracy_comparison(\n",
    "    mock_accuracy,\n",
    "    title=\"Accuracy Comparison by Prompt Technique\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Print rankings\n",
    "print(\"\\nAccuracy Rankings:\")\n",
    "for i, (tech, acc) in enumerate(sorted(mock_accuracy.items(), key=lambda x: x[1], reverse=True), 1):\n",
    "    print(f\"  {i}. {tech}: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss comparison\n",
    "fig = plot_loss_comparison(\n",
    "    mock_loss,\n",
    "    title=\"Loss Function Comparison (Lower is Better)\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Print rankings\n",
    "print(\"\\nLoss Rankings (Lower is Better):\")\n",
    "for i, (tech, loss) in enumerate(sorted(mock_loss.items(), key=lambda x: x[1]), 1):\n",
    "    print(f\"  {i}. {tech}: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validator\n",
    "validator = StatisticalValidator(alpha=0.05)\n",
    "\n",
    "# Calculate confidence intervals\n",
    "ci = validator.calculate_confidence_intervals(mock_accuracy_dist, confidence=0.95)\n",
    "\n",
    "print(\"95% Confidence Intervals:\")\n",
    "for tech, intervals in ci.items():\n",
    "    print(f\"\\n{tech}:\")\n",
    "    print(f\"  Mean: {intervals['mean']:.3f}\")\n",
    "    print(f\"  CI: [{intervals['lower']:.3f}, {intervals['upper']:.3f}]\")\n",
    "    print(f\"  Std Error: {intervals['std_error']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confidence intervals\n",
    "means = {tech: ci[tech]['mean'] for tech in techniques}\n",
    "lowers = {tech: ci[tech]['lower'] for tech in techniques}\n",
    "uppers = {tech: ci[tech]['upper'] for tech in techniques}\n",
    "\n",
    "fig = plot_confidence_intervals(\n",
    "    means, lowers, uppers,\n",
    "    title=\"Accuracy with 95% Confidence Intervals\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise statistical comparison\n",
    "comparison = validator.compare_techniques(\n",
    "    {tech: {'accuracy': vals} for tech, vals in mock_accuracy_dist.items()},\n",
    "    metric='accuracy',\n",
    "    use_parametric=True\n",
    ")\n",
    "\n",
    "print(\"Pairwise Statistical Tests (with Bonferroni correction):\")\n",
    "print(f\"\\nAlpha: {comparison['alpha']}\")\n",
    "print(f\"Bonferroni-corrected alpha: {comparison['bonferroni_alpha']:.4f}\")\n",
    "print(\"\\nSignificant differences:\")\n",
    "for pair, result in comparison['pairwise_tests'].items():\n",
    "    if result['significant']:\n",
    "        print(f\"  {pair}: p={result['p_value']:.4f} ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overall Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate composite scores (weighted accuracy and loss)\n",
    "composite_scores = {}\n",
    "for tech in techniques:\n",
    "    # Higher accuracy is better, lower loss is better\n",
    "    # Normalize and combine\n",
    "    score = 0.6 * mock_accuracy[tech] + 0.4 * (1 - mock_loss[tech])\n",
    "    composite_scores[tech] = score\n",
    "\n",
    "# Assign rankings\n",
    "rankings = {}\n",
    "for i, (tech, score) in enumerate(sorted(composite_scores.items(), key=lambda x: x[1], reverse=True), 1):\n",
    "    rankings[tech] = i\n",
    "\n",
    "# Plot rankings\n",
    "fig = plot_technique_rankings(\n",
    "    rankings, composite_scores,\n",
    "    title=\"Overall Technique Rankings (Composite Score)\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for tech in techniques:\n",
    "    summary_data.append({\n",
    "        'Technique': tech.replace('_', ' ').title(),\n",
    "        'Accuracy': f\"{mock_accuracy[tech]:.3f}\",\n",
    "        'Loss': f\"{mock_loss[tech]:.3f}\",\n",
    "        'Composite Score': f\"{composite_scores[tech]:.3f}\",\n",
    "        'Rank': rankings[tech],\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "df_summary = df_summary.sort_values('Rank')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "Based on the mock data:\n",
    "\n",
    "1. **Best Performing Technique**: Chain-of-Thought++ (CoT++)\n",
    "   - Highest accuracy: 89%\n",
    "   - Lowest loss: 0.20\n",
    "   - Includes self-verification and confidence scoring\n",
    "\n",
    "2. **Runner-ups**:\n",
    "   - Tree-of-Thoughts (ToT): 87% accuracy\n",
    "   - Chain-of-Thought (CoT): 85% accuracy\n",
    "\n",
    "3. **Baseline Performance**: 72% accuracy\n",
    "   - All optimization techniques outperform baseline\n",
    "   - Improvement range: 6-17 percentage points\n",
    "\n",
    "4. **Statistical Significance**:\n",
    "   - Top techniques show statistically significant improvements\n",
    "   - Bonferroni correction applied for multiple comparisons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
