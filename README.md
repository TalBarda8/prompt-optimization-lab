# Prompt Optimization & Evaluation System

A comprehensive system for demonstrating measurable performance improvements through systematic prompt engineering using information-theoretic metrics and statistical validation.

## Overview

This project implements a rigorous experimental framework to:
- Evaluate 6+ prompt engineering techniques (CoT, CoT++, ReAct, ToT, Role-Based, Few-Shot)
- Measure improvements using entropy, perplexity, and accuracy metrics
- Validate results with statistical significance testing (p < 0.05)
- Generate publication-quality visualizations and reports

## Project Structure

```
prompt-optimization-lab/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ PRD.md                       # Complete Product Requirements Document
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env.example                 # Environment variables template
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pipeline_config.yaml    # Pipeline configuration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset_a.json          # Simple QA dataset (75 samples)
â”‚   â””â”€â”€ dataset_b.json          # Multi-step reasoning dataset (35 samples)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                   # Dataset creation and validation
â”‚   â”œâ”€â”€ prompts/                # Prompt engineering techniques
â”‚   â”œâ”€â”€ evaluation/             # Metrics and statistical tests
â”‚   â”œâ”€â”€ visualization/          # Graph generation
â”‚   â”œâ”€â”€ llm/                    # LLM client with caching
â”‚   â””â”€â”€ pipeline/               # End-to-end orchestration
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for analysis
â”œâ”€â”€ tests/                       # Unit and integration tests
â”œâ”€â”€ results/                     # Experimental results
â””â”€â”€ figures/                     # Generated visualizations
```

## Installation

### Prerequisites
- Python 3.9, 3.10, or 3.11
- OpenAI API key (for GPT-4) or Anthropic API key (for Claude)
- 8GB+ RAM recommended
- 10GB+ free disk space

### Setup

1. Clone the repository:
```bash
git clone https://github.com/TalBarda8/prompt-optimization-lab.git
cd prompt-optimization-lab
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Set up environment variables:
```bash
cp .env.example .env
# Edit .env and add your API key
```

## Usage

### Run the complete pipeline:
```bash
python main.py
```

### Run specific phases:
```bash
# Phase 1: Data Preparation
python main.py --phase data

# Phase 2: Baseline Evaluation
python main.py --phase baseline

# Phase 3: Prompt Optimization
python main.py --phase optimization

# Phase 4: Statistical Comparison
python main.py --phase evaluation

# Phase 5: Visualization
python main.py --phase visualization
```

### Interactive Analysis:
```bash
jupyter notebook notebooks/
```

## Configuration

Edit `config/pipeline_config.yaml` to customize:
- LLM provider and model
- Optimization techniques to test
- Loss function weights (Î±, Î², Î³, Î´)
- Statistical test parameters

## Results

After running the pipeline, results will be available in:
- `results/baseline/` - Baseline evaluation metrics
- `results/optimized/` - Optimized prompt results
- `results/final_report.pdf` - Comprehensive analysis report
- `figures/` - All 12 required visualizations

## Key Metrics

The system evaluates prompts using:
- **Accuracy**: Task performance (with fuzzy matching)
- **Entropy H(Y|X)**: Output uncertainty (lower is better)
- **Perplexity**: Model confidence (lower is better)
- **Loss Function**: L = Î±Â·H + Î²Â·Length + Î³Â·PPL + Î´Â·(1-Acc)

## Success Criteria (from PRD)

- âœ… Statistically significant improvement (p < 0.05)
- âœ… Minimum 15% accuracy improvement over baseline
- âœ… 20%+ entropy reduction on average
- âœ… Publication-ready visualizations and documentation

## Testing

Run the test suite:
```bash
pytest tests/ --cov=src --cov-report=html
```

## Documentation

- **PRD.md**: Complete product requirements specification
- **API Documentation**: See `docs/api.md` (generated)
- **Jupyter Notebooks**: Step-by-step analysis in `notebooks/`

## License

MIT License - See LICENSE file for details

## Citation

If you use this system in your research, please cite:

```bibtex
@software{prompt_optimization_2025,
  title={Prompt Optimization \& Evaluation System},
  author={AI Systems Engineering Team},
  year={2025},
  url={https://github.com/TalBarda8/prompt-optimization-lab}
}
```

## Contact

For questions or issues, please open a GitHub issue or contact the development team.

---

**Status**: ðŸš§ Under Development (Stage 0/13 Complete)

**Last Updated**: 2025-12-11
